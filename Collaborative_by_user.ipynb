{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "max_err = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ui_matrix = pickle.load(open('./Models/ui_matrix_ua.pkl', 'rb'))\n",
    "except:\n",
    "    df = pd.read_csv('./ml-100k/ua.base', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "    ui_matrix = df.pivot(index='user_id', columns='item_id', values='rating')\n",
    "    ui_matrix.fillna(0, inplace=True)\n",
    "\n",
    "pickle.dump(ui_matrix, open('./Models/ui_matrix_ua.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    user_similarity = pickle.load(open('./Models/user_similarity_ua.pkl', 'rb'))\n",
    "except:\n",
    "    user_similarity = cosine_similarity(ui_matrix)\n",
    "    user_similarity = pd.DataFrame(user_similarity, index=ui_matrix.index, columns=ui_matrix.index)\n",
    "\n",
    "pickle.dump(user_similarity, open('./Models/user_similarity_ua.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of record in ui_matrix which column i is not 0\n",
    "def users_who_rated_for_item(ui_matrix: pd.DataFrame, i):\n",
    "    user_ids = ui_matrix[ui_matrix[i] != 0].index\n",
    "    rates = ui_matrix[ui_matrix[i] != 0][i]\n",
    "    ans = pd.DataFrame({'user_id': user_ids, 'rating': rates})\n",
    "    return ans\n",
    "\n",
    "# similarity list of user u and users who rated for item i\n",
    "def similarity_list(user_similarity, ui_matrix, u, i):\n",
    "    return user_similarity.loc[u, users_who_rated_for_item(ui_matrix, i)['user_id']]\n",
    "\n",
    "# predict rating of user u for item i by top k similar users\n",
    "def predict_rating_by_top_k(user_similarity, ui_matrix, u, i, k) -> float:\n",
    "    if u not in ui_matrix.index or i not in ui_matrix.columns: return -1\n",
    "    if ui_matrix.loc[u, i] != 0:\n",
    "        return ui_matrix.loc[u, i]\n",
    "    df = users_who_rated_for_item(ui_matrix, i)\n",
    "    df['similarity'] = similarity_list(user_similarity, ui_matrix, u, i)\n",
    "    df = df.sort_values(by='similarity', ascending=False)\n",
    "    df = df.iloc[:k, :]\n",
    "    return (df['similarity'] * df['rating']).sum() / df['similarity'].sum()\n",
    "\n",
    "# items which user u may interested in\n",
    "def recommend_items(user_similarity, ui_matrix, u, k):\n",
    "    assert u in ui_matrix.index and k in range(1, ui_matrix.shape[0])\n",
    "    pred_rate = [predict_rating_by_top_k(user_similarity, ui_matrix, u, i, k) for i in ui_matrix.columns]\n",
    "    ans = pd.DataFrame({'item_id': ui_matrix.columns, 'rating': pred_rate})\n",
    "    ans = ans.sort_values(by='rating', ascending=False)\n",
    "    return ans.iloc[:k, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./ml-100k/ua.test', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "test_df['pred_rating'] = [predict_rating_by_top_k(user_similarity, ui_matrix, u, i, k) for u, i in zip(test_df['user_id'], test_df['item_id'])]\n",
    "test_df['pred_rating_rounded'] = test_df['pred_rating'].apply(lambda x: round(x))\n",
    "test_df['err'] = test_df['pred_rating'] - test_df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8444326617179215\n"
     ]
    }
   ],
   "source": [
    "errs = test_df['err'].to_numpy()\n",
    "good_err = errs[abs(errs) < max_err]\n",
    "acc = len(good_err) / len(test_df)\n",
    "print(f'Accuracy: {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
